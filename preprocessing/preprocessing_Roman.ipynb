{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO:\n",
    "- Describe HBc.. Columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4Da Medical Data Analysis and Visualisation FS\n",
    "**Authors:** Roman Studer, Alexandre Rau\n",
    "\n",
    "**Goal:** The goal of the Medical Challenge is the automated classification of the eye disease uveities. The goal is to find the best possible model for classifying the disease based on a data set of +1000 patients. Important features for the prediction are to be identified."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detailed Description\n",
    "The detailed description was taken from the job description on the [DS-Spaces website](https://ds-spaces.technik.fhnw.ch/medical-data-analysis-and-visualisation-fs/#menu-second) on 08/03/2021: \n",
    "\n",
    "In this international challenge, you will work in collaboration with Prof. Dr. Nida Şen (MD, MHS, Director of the Uveitis Clinic at the National Eye Institute, Washington DC) and her team, including Dr. Shilpa Kodati. Dr. David Kuo (MD, Biomed Eng.) of University of California, San Diego is also glad to support to project (in an asynchronous manner due to the large differences in time zones). You can listen to a 25-min podcast interview with Dr. Şen to get a first impression of her research agenda. Using real data obtained from 1075 patients with 55 markers (more  detailed description below), you will delve into computational methods for medical research to identify which markers are relevant for the diagnosis of uveitis, a common eye disease. \n",
    "\n",
    "**International and interdisciplinary** setting As the description above suggests, an exciting and challenging aspect of this challenge is in its interdisciplinarity and internationality. You will experience different academic cultures, professional roles/backgrounds and fully practice your English in a professional setting. The collaborative international work means several virtual meetings with our international partner and requires some flexibility due to time zones. These interactions will serve as an opportunity to practice and reflect on questions related to intercultural competence, i.e., intercultural communication and collaboration. Since it is a medical challenge you will also need to learn new terminology and communicate your ideas to a client of a different field.\n",
    "\n",
    "**Description of the disease**  Uveitis is a group of inflammatory diseases of the uveal tract of the eye. It is a sight-threatening autoimmune disease and it is responsible for approximately 10-15% of blindness. This disease has a real and sizable impact: Besides its life-changing negative implications for the individual, it also has consequences for the society, because it affects people in their most productive work years and thus leads to a socioeconomic burden. Uveitis is a multifactorial condition and its causes are not fully understood despite recent advances. Challenges in clinical uveitis research include disease heterogeneity, lack of understanding of what triggers and what propagates the disease. \n",
    "\n",
    "## Tasks\n",
    "Imagine that you are a part of a scientific team, working in the team’s data science unit. The above-mentioned dataset, already pre-processed & cleaned, is delivered to you with the following request: Using an exploratory (agnostic) data science and machine learning approach, analyze which of the variables might \n",
    "\n",
    "a) have strong correlations with each other, and with the diagnosed diseases of patients and patient groups\n",
    "\n",
    "b) lead to the identification (prediction) of the uveitis subtypes which may inform eventual diagnosis\n",
    "\n",
    "c) compare the HLA haplotypes in healthy controls to those in the project data, check if they correlate with specific subgroups. This will help verifying genetic predictors of certain subtypes of disease in this uveitis cohort. Data from healthy controls, i.e., normative data is publicly available as a separate data set (also see “Data” section below)\n",
    "\n",
    "d) analyze if the steps in the preprocessing could be improved\n",
    "\n",
    "e) which missing value strategy is best.\n",
    "\n",
    "We expect that you use a combination of exploratory data analysis (see competence ‘eda‘) probability testing, machine learning (see competences ule, sul) and visual analytics (see competence ‘van‘) to conduct your analyses. Using machine learning (ML), you will search for patterns and anomalies in the data, profile patients and/or patient groups (e.g., genetic profiles or patient history, their state vs. normative baseline data). Using visual analytics (VA) you will visualize/plot to demonstrate the relationships between variables and what ML detects using multiple linked views.\n",
    "\n",
    "The project management will be inspired in scrum. Code and artifacts should be documented in a git repository. After 1 or 2 weeks in the project a proposal of focus of the challenge group should be submitted to the challenge owners, preferably with a rough roadmap.\n",
    "\n",
    "As a data scientist, you will need to interpret and give context to your findings in order to enhance their value.\n",
    "\n",
    "You may also explore (optional) visualizing ML algorithm’s decisions (as a meta tool to make ML “understandable”). Such an analysis will facilitate the dialogue with the domain expert when analyzing which factor was (potentially) responsible for which impact. Note that understandable (explainable, interpretable) machine learning is a large and advanced topic. If you are curious about it, a good starting point maybe to learn about the decision tree concept: Decision Tree for starters is although fine: https://www.youtube.com/watch?v=qB8HZpwqPEg\n",
    "\n",
    "We will guide you both by providing you some materials from this space and guiding you to the relevant competences. When you have suggestions and questions, please use the “Stream” space, also answer each other’s questions, come to our sessions (see “timelines”), or when it is really needed, request appointments. Main tools for communication should be the “Stream” and the provided office hours.\n",
    "\n",
    "## Approach\n",
    "1. Exploratory data analysis (EDA)\n",
    "\n",
    "This inital step acts as the foundation of the project. By analyzing the data set, initial questions of understanding can be clarified and necessary steps for preparing the data set can be found. Strategies for imputation of missing, btw. wrong values are also considered here.\n",
    "\n",
    "2. Data Preprocessing\n",
    "\n",
    "In parallel with the EDA process, the first transformations of the data are carried out. This includes, for example, normalization and simplification of column names, adaptation of the data types of individual features and elimination of entry errors.\n",
    "\n",
    "3. Feature extraction\n",
    "\n",
    "The data set is not in a \"Tidy Data\" state. This means that there are columns that contain more than one piece of information, i.e. more than one feature. Furthermore, columns exist that have values which should be transferred to more than one column. This step is necessary to prepare the dataset for a preprocessing pipeline that allows to transform the dataset for a machine learning model.\n",
    "\n",
    "4. Preprocessing pipeline\n",
    "\n",
    "At this point, the dataset was prepared in such a way that, with the support of the library sklearn.preprocessing, the dataset can be processed in such a way that a machine learning algorithm can work with it. \n",
    "\n",
    "5. Setup modular environment to test multiple algorithms\n",
    "\n",
    "By using the pipeline module of the sklearn library, we can build a modular environment that allows us to quickly apply and evaluate different machine learning algorithms.\n",
    "\n",
    "6. Identify useful models\n",
    "\n",
    "Of the models tested, the most promising can be taken out. Through parameter optimization, these can be refined to achieve higher accuracy. \n",
    "\n",
    "7. Identify useful features\n",
    "\n",
    "Depending on the algorithm, the influence of a feature is identifiable. This influence of individual features can be checked or increased by various methods.\n",
    "\n",
    "8. Document findings\n",
    "\n",
    "The insights gained will be recorded in a conference paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# imports, libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import regex as re\n",
    "from sklearn.preprocessing import StandardScaler, Binarizer, LabelEncoder, Normalizer, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# import helperfunctions\n",
    "import pipe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import and Renaming\n",
    "The dataset is imported with pandas `read_excel()`. The naming of the features, i.e. the names of the columns is not uniform. The features are renamed with the function `pipe.rename()`, which can be found in the script pipe.py, based on a given list. The list can be consulted in the document \"col_names&data_type-Copy1.xlsx\". All features are renamed in lowercase, and preceding and trailing spaces are removed. Brackets and their contents, e.g. \"(Blood)\", are removed. These would only complicate the readability of the code and are recognizable from the context as well as the name of the feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dataframe\n",
    "df = pd.read_excel(\"../data/uveitis_data.xlsx\")\n",
    "assert len(df) >= 1075, \"Data is not complete\"\n",
    "\n",
    "# rename columns\n",
    "df = pipe.rename(df, \"../data/col_names&data_type-Copy1.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dtype_transform(df, path):\n",
    "    '''\n",
    "    Converts zero values recorded as text to NaN's. \n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    '''\n",
    "    # remove leading or trailing whitespace in all categorical or text features\n",
    "    df = df.applymap(lambda x: x.strip() if isinstance(x, str) else x)\n",
    "    return df\n",
    "\n",
    "df = dtype_transform(df, \"../data/col_names&data_type-Copy1.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_to_nan():\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    df: df, Original DataFrame\n",
    "    path: str, Path to excel-file with dtypes linked to features\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    df: df, Returns original DataFrame with changed dtypes\n",
    "    '''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Categorical Features\n",
    "This section deals with categorical variables that can be taken as such directly from the dataset. There are features/variables that contain both categorical and numerical values. These are treated seperately. For each feature, a description is given of how it was processed. Mostly it is a simple normalization of the values, uniformization of values that contain the same information or removal of wrong or useless values. The decision to evaluate a value as \"missing\" is discussed in each case. All changes made can be adjusted or undone.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Description \n",
    "- **Gender**, a qualitative, nominal feature describing the patients gender. A patient can either be in the \"male\" or \"female\" category.\n",
    "\n",
    "- **Race** describes the patients ethnicity.\n",
    "\n",
    "- **Location** locates the position of the inflammation in the eye. A distinction is made between posterior, anterior, intermediate, etc. \n",
    "\n",
    "- The feature **Categorical** records the source of the inflammation as seen by the specialists who recorded the data. Uveitis can be caused by systemic problems, infections, or often idopathic.\n",
    "\n",
    "- **EHR Diagnosos** is an electronic transmited diagnose, usually given beforehand by another doctor, that has had no knowledge about the lab tests and final diagnosis.\n",
    "\n",
    "- **Specific Diagnosis** is the diagnosis given by the team that collected the data. According to Dr. Nida Sen this is one of the most important outcome variables. This variable will be consired to be the target feature. \n",
    "\n",
    "- **AC Abn Od Cells and AC Abn Os Cells**. These qualitative, ordinal features describe the severity of the inflammation of the Anterior Chamber Cells (AC) in either the left eye (OS) or the right eye (OD). The inflammation can be rated as 0, +0.5, +1, +2, +3, +4. The higher the value the more severe the inflammation is. If either one of these values a patient can be considered as \"Active\", else as \"Quiet\". This information could be recorded in a new column.\n",
    "\n",
    "- **Vit Abn Od Cells, Vit Abn Os Cells, Vit Abn Od Haze and Vit Abn Os Haze** describe (similar to AC Abn O...) the inflammation of cells in the left (OS) and right (OD) eye. The same scale of 0, +0.5, +1, +2, +3, +4 is used. If one of the values is higher than 0 the patient is considered to be \"Active\" as well. This information can be recorded in a new column as well. \n",
    "\n",
    "- **HBc (HepB core) Ab (Blood), HBs (HepB surface) Ag (Blood), HCV (HepC) Ab (Blood)** \n",
    "\n",
    "Features that contain categorical and numerical information will be discussed in a later chapter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gender\n",
    "\n",
    "This features containes the gender of the patient (\"female\" or \"male\") and is currently of the data type 'Object' ('O'). This feature gets transfromed to the dtype 'catgory' via the `pd.DataFrame.astype('category')`-function. This way it can later on easily be OneHotEncoded. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.gender.unique().tolist() # categories in feature 'gender'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.gender.dtype # dtype before transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gender\n",
    "df.gender = df.gender.astype('category')\n",
    "df.gender.dtype # dtype after transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Feature Race**\n",
    "The categorical variable \"Race\" includes the category \"race or ethnic group data not provided by source\". These values are treated as missing values, since they do not contain any information about the respective person. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.race = df.race.replace('Race or Ethnic Group Data Not Provided by Source',np.NaN)\n",
    "df.race = df.race.astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['loc'] = df['loc'].str.lower().str.strip()\n",
    "df[df['loc'] == 'pan'] = 'panuveitis'\n",
    "df['loc'] = df['loc'].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.cat = df.cat.str.lower().str.strip().astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.specific_diagnosis = df.specific_diagnosis.str.lower().astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.notes.value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "todo: describe change with 'c'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = ['ac_abn_od_cells', 'ac_abn_os_cells', 'vit_abn_od_cells',\n",
    "       'vit_abn_os_cells', 'vit_abn_od_haze', 'vit_abn_os_haze']\n",
    "for c in col: \n",
    "    # replace 'C' (for missing) with NaN\n",
    "    df[c] = df[c].replace('C',np.nan)\n",
    "    df[c] = df[c].astype('float')\n",
    "    df[c] = pd.Categorical(values=df[c], categories=df[c].unique().sort(), ordered=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = ['hbc__ab', 'hbs__ag', 'hcv__ab']\n",
    "for c in col:\n",
    "    df[c] = df[c].str.lower()\n",
    "    df.loc[df[c] == 'negative', c] = 0\n",
    "    df.loc[df[c] == 'see note | positive result s/co ratio is >5.0.  confirmatory testing i', c] = 1\n",
    "    df.loc[df[c] == 'see below | positive result s/co ratio is >5.0.  confirmatory testing', c] = 1\n",
    "    df.loc[df[c] == 'reactive', c] = 1\n",
    "    df.loc[df[c] == 'repeat reactive', c] = 1\n",
    "    df.loc[df[c] == 'invalid result', c] = np.nan\n",
    "    df.loc[df[c] == 'note:', c] = np.nan\n",
    "    df[c] = df[c].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_features = []\n",
    "categorical_features = []\n",
    "imputer = {'categorical':{'strategy':'constant', 'fill_value':'missing'}, 'numerical':{'strategy':'median'}}\n",
    "# preprocessor = pipe.preprocessing(categorical_features, numeric_features, imputer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.cat.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_range(data, feat, verbose=False):\n",
    "    data = pipe.extract_num(data, feat, verbose=verbose)\n",
    "    sns.set(rc={'figure.figsize':(9,6)})\n",
    "    sns.set_style(\"ticks\")\n",
    "    \n",
    "    # extract uom and ranges for feat\n",
    "    sub_data = data.loc[:,feat:].iloc[:,:3]\n",
    "    sub_range = sub_data.loc[:,sub_data.columns[sub_data.columns.str.contains(pat = 'range')]]\n",
    "    # plot boxplot for every \"range\" and corresponding uom\n",
    "    g = sns.boxplot(x=sub_range.iloc[:,0],y=sub_data[feat])\n",
    "    # overlay min and max range\n",
    "    ranges = [x for x in sub_range.iloc[:,0].unique() if str(x) != 'nan']\n",
    "    n_range = len(ranges)\n",
    "    for num, rang in enumerate(ranges):\n",
    "        if isinstance(rang, str):\n",
    "            min_range, max_range = rang.split('-')\n",
    "            min_range, max_range = float(min_range), float(max_range)\n",
    "            g.axhline(min_range, xmin=num/n_range+.05, xmax=num/n_range+(1/n_range-.05), ls='--')\n",
    "            g.axhline(max_range, xmin=num/n_range+.05, xmax=num/n_range+(1/n_range-.05), ls='--')\n",
    "            g.text(num+0.25,max_range+0.05, \"max.\")\n",
    "            g.text(num+0.25,min_range+0.05, \"min.\")\n",
    "            \n",
    "    \n",
    "    # Title, Subtitle and Axis\n",
    "    g.text(x=0.5, \n",
    "            y=1.08, \n",
    "            s=f'Ranges of Feature \"{feat}\"', \n",
    "            fontsize=10, weight='bold', ha='center', va='bottom', transform=g.transAxes)\n",
    "    g.text(x=0.5, \n",
    "            y=1.01, \n",
    "            s=f'Data has been transformed beforehand, nonnumerical values are excluded in this visualisation\\n min. and max. represent the range of values a test can produce', \n",
    "            fontsize=10, alpha=0.75, ha='center', va='bottom', transform=g.transAxes)\n",
    "    plt.xlabel(f'Range in [{sub_data.iloc[1,1]}]')\n",
    "    plt.ylabel(f'{feat} [{sub_data.iloc[1,1]}]')\n",
    "\n",
    "    \n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "plot_range(df, \"calcium\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_range(df, \"lactate_dehydrogenase\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
