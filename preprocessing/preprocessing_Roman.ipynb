{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO:\n",
    "- Describe HBc.. Columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4Da Medical Data Analysis and Visualisation FS\n",
    "**Authors:** Roman Studer, Alexandre Rau\n",
    "\n",
    "**Goal:** The goal of the Medical Challenge is the automated classification of the eye disease uveities. The goal is to find the best possible model for classifying the disease based on a data set of +1000 patients. Important features for the prediction are to be identified."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detailed Description\n",
    "The detailed description was taken from the job description on the [DS-Spaces website](https://ds-spaces.technik.fhnw.ch/medical-data-analysis-and-visualisation-fs/#menu-second) on 08/03/2021: \n",
    "\n",
    "In this international challenge, you will work in collaboration with Prof. Dr. Nida Şen (MD, MHS, Director of the Uveitis Clinic at the National Eye Institute, Washington DC) and her team, including Dr. Shilpa Kodati. Dr. David Kuo (MD, Biomed Eng.) of University of California, San Diego is also glad to support to project (in an asynchronous manner due to the large differences in time zones). You can listen to a 25-min podcast interview with Dr. Şen to get a first impression of her research agenda. Using real data obtained from 1075 patients with 55 markers (more  detailed description below), you will delve into computational methods for medical research to identify which markers are relevant for the diagnosis of uveitis, a common eye disease. \n",
    "\n",
    "**International and interdisciplinary** setting As the description above suggests, an exciting and challenging aspect of this challenge is in its interdisciplinarity and internationality. You will experience different academic cultures, professional roles/backgrounds and fully practice your English in a professional setting. The collaborative international work means several virtual meetings with our international partner and requires some flexibility due to time zones. These interactions will serve as an opportunity to practice and reflect on questions related to intercultural competence, i.e., intercultural communication and collaboration. Since it is a medical challenge you will also need to learn new terminology and communicate your ideas to a client of a different field.\n",
    "\n",
    "**Description of the disease**  Uveitis is a group of inflammatory diseases of the uveal tract of the eye. It is a sight-threatening autoimmune disease and it is responsible for approximately 10-15% of blindness. This disease has a real and sizable impact: Besides its life-changing negative implications for the individual, it also has consequences for the society, because it affects people in their most productive work years and thus leads to a socioeconomic burden. Uveitis is a multifactorial condition and its causes are not fully understood despite recent advances. Challenges in clinical uveitis research include disease heterogeneity, lack of understanding of what triggers and what propagates the disease. \n",
    "\n",
    "## Tasks\n",
    "Imagine that you are a part of a scientific team, working in the team’s data science unit. The above-mentioned dataset, already pre-processed & cleaned, is delivered to you with the following request: Using an exploratory (agnostic) data science and machine learning approach, analyze which of the variables might \n",
    "\n",
    "a) have strong correlations with each other, and with the diagnosed diseases of patients and patient groups\n",
    "\n",
    "b) lead to the identification (prediction) of the uveitis subtypes which may inform eventual diagnosis\n",
    "\n",
    "c) compare the HLA haplotypes in healthy controls to those in the project data, check if they correlate with specific subgroups. This will help verifying genetic predictors of certain subtypes of disease in this uveitis cohort. Data from healthy controls, i.e., normative data is publicly available as a separate data set (also see “Data” section below)\n",
    "\n",
    "d) analyze if the steps in the preprocessing could be improved\n",
    "\n",
    "e) which missing value strategy is best.\n",
    "\n",
    "We expect that you use a combination of exploratory data analysis (see competence ‘eda‘) probability testing, machine learning (see competences ule, sul) and visual analytics (see competence ‘van‘) to conduct your analyses. Using machine learning (ML), you will search for patterns and anomalies in the data, profile patients and/or patient groups (e.g., genetic profiles or patient history, their state vs. normative baseline data). Using visual analytics (VA) you will visualize/plot to demonstrate the relationships between variables and what ML detects using multiple linked views.\n",
    "\n",
    "The project management will be inspired in scrum. Code and artifacts should be documented in a git repository. After 1 or 2 weeks in the project a proposal of focus of the challenge group should be submitted to the challenge owners, preferably with a rough roadmap.\n",
    "\n",
    "As a data scientist, you will need to interpret and give context to your findings in order to enhance their value.\n",
    "\n",
    "You may also explore (optional) visualizing ML algorithm’s decisions (as a meta tool to make ML “understandable”). Such an analysis will facilitate the dialogue with the domain expert when analyzing which factor was (potentially) responsible for which impact. Note that understandable (explainable, interpretable) machine learning is a large and advanced topic. If you are curious about it, a good starting point maybe to learn about the decision tree concept: Decision Tree for starters is although fine: https://www.youtube.com/watch?v=qB8HZpwqPEg\n",
    "\n",
    "We will guide you both by providing you some materials from this space and guiding you to the relevant competences. When you have suggestions and questions, please use the “Stream” space, also answer each other’s questions, come to our sessions (see “timelines”), or when it is really needed, request appointments. Main tools for communication should be the “Stream” and the provided office hours.\n",
    "\n",
    "## Approach\n",
    "1. Exploratory data analysis (EDA)\n",
    "\n",
    "This inital step acts as the foundation of the project. By analyzing the data set, initial questions of understanding can be clarified and necessary steps for preparing the data set can be found. Strategies for imputation of missing, btw. wrong values are also considered here.\n",
    "\n",
    "2. Data Preprocessing\n",
    "\n",
    "In parallel with the EDA process, the first transformations of the data are carried out. This includes, for example, normalization and simplification of column names, adaptation of the data types of individual features and elimination of entry errors.\n",
    "\n",
    "3. Feature extraction\n",
    "\n",
    "The data set is not in a \"Tidy Data\" state. This means that there are columns that contain more than one piece of information, i.e. more than one feature. Furthermore, columns exist that have values which should be transferred to more than one column. This step is necessary to prepare the dataset for a preprocessing pipeline that allows to transform the dataset for a machine learning model.\n",
    "\n",
    "4. Preprocessing pipeline\n",
    "\n",
    "At this point, the dataset was prepared in such a way that, with the support of the library sklearn.preprocessing, the dataset can be processed in such a way that a machine learning algorithm can work with it. \n",
    "\n",
    "5. Setup modular environment to test multiple algorithms\n",
    "\n",
    "By using the pipeline module of the sklearn library, we can build a modular environment that allows us to quickly apply and evaluate different machine learning algorithms.\n",
    "\n",
    "6. Identify useful models\n",
    "\n",
    "Of the models tested, the most promising can be taken out. Through parameter optimization, these can be refined to achieve higher accuracy. \n",
    "\n",
    "7. Identify useful features\n",
    "\n",
    "Depending on the algorithm, the influence of a feature is identifiable. This influence of individual features can be checked or increased by various methods.\n",
    "\n",
    "8. Document findings\n",
    "\n",
    "The insights gained will be recorded in a conference paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# imports, libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import regex as re\n",
    "from sklearn.preprocessing import StandardScaler, Binarizer, LabelEncoder, Normalizer, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import os\n",
    "\n",
    "# import helperfunctions\n",
    "import pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import and Renaming\n",
    "The dataset is imported with pandas `read_excel()`. The naming of the features, i.e. the names of the columns is not uniform. The features are renamed with the function `pipe.rename()`, which can be found in the script pipe.py, based on a given list. The list can be consulted in the document \"col_names&data_type-Copy1.xlsx\". All features are renamed in lowercase, and preceding and trailing spaces are removed. Brackets and their contents, e.g. \"(Blood)\", are removed. These would only complicate the readability of the code and are recognizable from the context as well as the name of the feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dataframe\n",
    "df = pd.read_excel(\"../data/uveitis_data.xlsx\")\n",
    "assert len(df) >= 1075, \"Data is not complete\"\n",
    "\n",
    "# rename columns\n",
    "df = pipe.rename(df, \"../data/col_names&data_type-Copy1.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.applymap(lambda x: x.strip() if isinstance(x, str) else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pipe.drop_nan_columns(df, nan_percentage=.5, verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pipe.drop_via_filter(test, 'range', verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Categorical Features\n",
    "This section deals with categorical variables that can be taken as such directly from the dataset. There are features/variables that contain both categorical and numerical values. These are treated seperately. For each feature, a description is given of how it was processed. Mostly it is a simple normalization of the values, uniformization of values that contain the same information or removal of wrong or useless values. The decision to evaluate a value as \"missing\" is discussed in each case. All changes made can be adjusted or undone.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Description \n",
    "- **Gender**, a qualitative, nominal feature describing the patients gender. A patient can either be in the \"male\" or \"female\" category.\n",
    "\n",
    "- **Race** describes the patients ethnicity.\n",
    "\n",
    "- **Location** locates the position of the inflammation in the eye. A distinction is made between posterior, anterior, intermediate, etc. \n",
    "\n",
    "- The feature **Categorical** records the source of the inflammation as seen by the specialists who recorded the data. Uveitis can be caused by systemic problems, infections, or often idopathic.\n",
    "\n",
    "- **EHR Diagnosos** is an electronic transmited diagnose, usually given beforehand by another doctor, that has had no knowledge about the lab tests and final diagnosis.\n",
    "\n",
    "- **Specific Diagnosis** is the diagnosis given by the team that collected the data. According to Dr. Nida Sen this is one of the most important outcome variables. This variable will be consired to be the target feature. \n",
    "\n",
    "- **AC Abn Od Cells and AC Abn Os Cells**. These qualitative, ordinal features describe the severity of the inflammation of the Anterior Chamber Cells (AC) in either the left eye (OS) or the right eye (OD). The inflammation can be rated as 0, +0.5, +1, +2, +3, +4. The higher the value the more severe the inflammation is. If either one of these values a patient can be considered as \"Active\", else as \"Quiet\". This information could be recorded in a new column.\n",
    "\n",
    "- **Vit Abn Od Cells, Vit Abn Os Cells, Vit Abn Od Haze and Vit Abn Os Haze** describe (similar to AC Abn O...) the inflammation of cells in the left (OS) and right (OD) eye. The same scale of 0, +0.5, +1, +2, +3, +4 is used. If one of the values is higher than 0 the patient is considered to be \"Active\" as well. This information can be recorded in a new column as well. \n",
    "\n",
    "- **HBc (HepB core) Ab (Blood), HBs (HepB surface) Ag (Blood), HCV (HepC) Ab (Blood)** \n",
    "\n",
    "Features that contain categorical and numerical information will be discussed in a later chapter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gender\n",
    "\n",
    "This features containes the gender of the patient (\"female\" or \"male\") and is currently of the data type 'Object' ('O'). This feature gets transfromed to the dtype 'catgory' via the `pd.DataFrame.astype('category')`-function. This way it can later on easily be OneHotEncoded. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.gender.unique().tolist() # categories in feature 'gender'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.gender.dtype # dtype before transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gender\n",
    "def gender_dtype(df):\n",
    "    df.gender = df.gender.astype('category')\n",
    "    return df\n",
    "\n",
    "df = gender_dtype(df)\n",
    "df.gender.dtype # dtype after transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Race\n",
    "The categorical variable \"Race\" includes the category \"race or ethnic group data not provided by source\". These values are treated as missing values, aka in the category 'unknown', since they do not contain any information about the respective person. \"race or ethnic group data not provided by source\" and \"unknown race\" collaps into the category \"unknown\". Missing values (NaN's) are also marked with 'unknown'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_race(df):\n",
    "    df.race = df.race.replace({'Race or Ethnic Group Data Not Provided by Source':'unknown', \n",
    "                               'Unknown Race':'unknown'})\n",
    "    df.race = df.race.fillna(value='unknown')\n",
    "    df.race = df.race.astype('category')\n",
    "    assert df.race.isna().sum() == 0, 'Not all missing values are treated'\n",
    "    return df\n",
    "    \n",
    "df = preprocessing_race(df)\n",
    "df.race.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Categories with less than 10 values, aka 'Native Hawaiian or Other Pacific Islander', 'American Indian or Alaska Native' may should be collapsed or discarded,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### loc, \"Location\"\n",
    "The loc-Feature indicates the location of the inflammation of the eye. The category 'pan' is the same as 'panuveitis' and can be collapsed. \n",
    "We want to explore two diffrent approaches to treat this feature:\n",
    "\n",
    "1. We keep the categories 'anterior', 'intermediate', 'panuveitis', 'posterior' and 'sclerits'. All categories indicate a diffrent section of the eye (or multiple at once) that show inflammation. \n",
    "2. We collapse mutliple categories to get an 'anterior' and 'posterior' category. Aka, collapse the location to inflammations in the front and the back of the eye (binary feature). To achieve this we collapse the categories 'intermediate', 'posterior' and 'panuveities' to the category \"posterior_segment\". 'anterior' and 'scleritis' get collapsed to the category 'anterior_segment'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pipe.preprocessing_loc(df,'multi', verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cat, \"Category\" \n",
    "The cat-feature describes the origin of the inflammation. For example infectious or idiopathic origin. \n",
    "We can collapse the categories \"nonneoplastic masquerade\" and \" neoplastic masquerade\" to not_uveitits. As these are \"pseudo-uveitis\"-types. The row with the single occurance of scleritis should be dropped as it has to few records with this category. The single occurance of NaN is a \"not_uveitis\" case and can be filled with that category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pipe.preprocessing_cat(df)\n",
    "df.cat.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ehr_diagnosis\n",
    "EHR diagnosos is an electronic transmitted diagnosis, usually given beforehand by another doctor, not knowing about the lab results and final diagnosis. This feature contains a lot of diffrent categories (533 unique values). Because of that we drop this feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=['ehr_diagnosis'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### specific_diagnosis\n",
    "Specific diagnoses which occur less or equal to 10 times in the dataset get collapsed into the catgory 'other'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pipe.preprocessing_specific(df)\n",
    "df.specific_diagnosis.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### notes\n",
    "This column contains notes to the diagnosis and is mostly missing. This feature will be dropped at the end of the preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'notes' in df.columns:\n",
    "    print(df.note.isna().sum()/len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ac_abn_...-columns and vit_abn_...-columns\n",
    "Replace 'C' as Missing and change dtype to 'float'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_inflammation(df, col = ['ac_abn_od_cells', 'ac_abn_os_cells', 'vit_abn_od_cells',\n",
    "       'vit_abn_os_cells', 'vit_abn_od_haze', 'vit_abn_os_haze']):\n",
    "    for c in col: \n",
    "        # replace 'C' (for missing) with NaN\n",
    "        df[c] = df[c].replace('C',np.nan)\n",
    "        df[c] = df[c].astype('float')\n",
    "        df[c] = pd.Categorical(values=df[c], categories=df[c].unique().sort(), ordered=True)\n",
    "    return df\n",
    "df = preprocessing_inflammation(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### hbc__ab, hbs__ag and hcv__ab\n",
    "These columns encode the lab results for diffrent types of hepatitis. We encode these in binary form. Negative results are '0' and positive results get encoded as '1'. There are some cases where neither a positive or negative result can be identified. These values will be set as missing values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_hepatitis(df, col=['hbc__ab', 'hbs__ag', 'hcv__ab'], verbose=False):\n",
    "    for c in col:\n",
    "        df[c] = df[c].str.lower()\n",
    "        df.loc[df[c] == 'negative', c] = 0\n",
    "        df.loc[df[c] == 'see note | positive result s/co ratio is >5.0.  confirmatory testing i', c] = 1\n",
    "        df.loc[df[c] == 'see below | positive result s/co ratio is >5.0.  confirmatory testing', c] = 1\n",
    "        df.loc[df[c] == 'reactive', c] = 1\n",
    "        df.loc[df[c] == 'repeat reactive', c] = 1\n",
    "        df.loc[df[c] == 'invalid result', c] = np.nan\n",
    "        df.loc[df[c] == 'note:', c] = np.nan\n",
    "        df[c] = df[c].astype('category')\n",
    "        if verbose:\n",
    "            print(df[c].value_counts())\n",
    "    return df\n",
    "df = preprocessing_hepatitis(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### hla-columns\n",
    "These columns contain genetic data about the patients. This data should be used for a seperate model and thus will not be used (at least for now) and dropped. A function has been defined to drop these columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pipe.drop_via_filter(test, 'hla', verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features containing both numerical and categorical values\n",
    "Certain columns don't follow the tidy data principle that only one datatyp should be existant in a column/feature.\n",
    "This chapter deals with said columns and either splits them into a numeric and categorical feature or changes values to reach a uniform datatyp over a column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anti-CCP Ab\n",
    "Anti-CCP is a numeric column with mostly values set to '<20'. A value below or at 20 is viewed as a negative result. Above 20 the result is positive. This allows for a binarization of the column. We set every value below or at 20 to 0 (aka 'negative') and all values above 20 to 1 (aka postive). Some values are still missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pipe.num_to_binary(df, 'anti-ccp_ab', 20)\n",
    "# df['anti-ccp_ab'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anti-ENA Screen\n",
    "Anti-ENA Screen consists of mostly 'NEG' (Negative) Values (1001 out of 1075), we assume that the other, numerical values can be regarded as positive. We encode these into 0 (Negative) and 1 (Positive) values. The singel occurance of 'see note | In-house test down.  Test re-ordered and sent to Referral L' gets dropped and replaced with `np.nan`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['anti-ena_screen'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Antinuclear Antibody"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['antinuclear_antibody'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DNA Double-Stranded Ab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['dna_double-stranded_ab'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `pipe.neg_col_to_cat` transforms a list of columns (in our case `['anti-ena_screen','antinuclear_antibody','dna_double-stranded_ab']`) to binary, categorical columns where 0 = 'Negative' and 1 = 'Postive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `pipe.neg_col_to_cat` changes the features 'anti-ccp_ab', 'anti-ena_screen', 'antinuclear_antibody', and 'dna_double-stranded_ab' to categorical variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pipe.neg_col_to_cat(df, ['anti-ccp_ab','anti-ena_screen','antinuclear_antibody','dna_double-stranded_ab', 'rheumatoid_factor'], verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rheumatoid_factor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Myeloperoxidase Ab\n",
    "This column has been dropped because of to many missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proteinase-3 Antibodies\n",
    "This column has been dropped because of to many missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['proteinase-3_antibodies'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numerical Features\n",
    "This section deals with numerical variables that can be extracted from the dataset. As mentioned before, there are features/variables that contain both categorical and numerical values. These are treated seperately. For each feature, a description is given of how it was processed. Mostly it is a simple normalization of the values, uniformization of values that contain the same information or removal of wrong or useless values. The decision to evaluate a value as \"missing\" is discussed in each case. All changes made can be adjusted or undone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#variables assignement\n",
    "a = df.copy()\n",
    "a = pipe.preprocessing_numeric(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Handle multiple Units of Measurment in Feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pipe.uom_fix(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numeric Features with ranges to categorical\n",
    "We propose to encode features that have a range of \"normal\" values associated with them to categorical features. Values in the range are considered 'normal'. We thus create three categories: 0 = 'below normal', 1 = 'normal', 2 = 'above normal.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = df.copy()\n",
    "t = pipe.preprocessing_numeric(t, num_to_cat = True, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop 'uom' and 'range' columns\n",
    "Every lab test is accompanied by two columns. One specifies the unit of measurement (uom) for said test and the other defines the acceptable/normal range of the test (range).\n",
    "Although these informations are important for the exploratory data analysis test and the preprocessing it is not advised to include these columns in the dataframe that serves as the input for a machine learning algorithmn. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pipe.drop_uom_and_range(df, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CHECK IF EVERY COLUMN IS ACCOUNTED FOR:\n",
    "- 'id'\n",
    "- [x] 'gender' \n",
    "- [x] 'race'\n",
    "- [x] 'loc'\n",
    "- [x] 'ehr_diagnosis' (Dropped)\n",
    "- [x] 'anti-dnase_b' (Dropped, too many missing values)\n",
    "- [x] 'other_' (Dropped, too many missing values)\n",
    "- [x] 'notes' (Dropped, too many missing values)\n",
    "- [x] 'beta-2-microglobulin' (Dropped, too many missing values)\n",
    "- [x] 'lupus_anticoagulant' (Dropped, too many missing values)\n",
    "- [x] 'myeloperoxidase_ab' (Dropped, too many missing values)\n",
    "- [x] 'proteinase-3_antibodies' (Dropped, too many missing values)\n",
    "- [x] 'cat'\n",
    "- [x] 'specific_diagnosis'\n",
    "- [x] 'ac_abn_od_cells'\n",
    "- [x] 'ac_abn_os_cells'\n",
    "- [x] 'vit_abn_od_cells'\n",
    "- [x] 'vit_abn_os_cells'\n",
    "- [x] 'vit_abn_od_haze'\n",
    "- [x] 'vit_abn_os_haze'\n",
    "- 'calcium'\n",
    "- 'lactate_dehydrogenase'\n",
    "- 'c-reactive_protein,_normal_and_high_sensitivity'\n",
    "- 'wbc'\n",
    "- 'rbc'\n",
    "- 'hemoglobin'\n",
    "- 'hematocrit'\n",
    "- 'mcv'\n",
    "- 'mch'\n",
    "- 'mchc'\n",
    "- 'rdw'\n",
    "- 'platelet_count'\n",
    "- 'neutrophil_%'\n",
    "- 'lymphocytes_%'\n",
    "- 'angiotensin_conv#enzyme'\n",
    "- 'lysozyme,_plasma'\n",
    "- 'anti-ccp_ab'\n",
    "- 'anti-ena_screen'\n",
    "- 'antinuclear_antibody'\n",
    "- 'complement_c3'\n",
    "- 'complement_c4'\n",
    "- 'dna_double-stranded_ab'\n",
    "- [x] 'hla-a*' (Dropped)\n",
    "- [x] 'hla_a_1' (Dropped)\n",
    "- [x] 'hla_a_2' (Dropped)\n",
    "- [x] 'hla-b*' (Dropped)\n",
    "- [x] 'hla_b_1' (Dropped)\n",
    "- [x] 'hla_b_2' (Dropped)\n",
    "- [x] 'hla-cw*' (Dropped)\n",
    "- [x] 'hla_c_1' (Dropped)\n",
    "- [x] 'hla_c_2' (Dropped)\n",
    "- [x] 'hla-drb1*' (Dropped)\n",
    "- [x] 'hla_drb1_1' (Dropped)\n",
    "- [x] 'hla_drb1_2' (Dropped)\n",
    "- [x] 'hla-dqb1*_/_dq*' (Dropped)\n",
    "- [x] 'hla_dq_1' (Dropped)\n",
    "- [x] 'hla_dq_2' (Dropped)\n",
    "- [x] 'hla-drb_*' (Dropped)\n",
    "- [x] 'hla_drb*_1' (Dropped)\n",
    "- [x] 'hla_drb*_2' (Dropped)\n",
    "- 'rheumatoid_factor'\n",
    "- [x] 'hbc__ab'\n",
    "- [x] 'hbs__ag'\n",
    "- [x] 'hcv__ab'\n",
    "- [x] uom and range columns (Dropped, after used for transformation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def preprocessing_pipe(num_to_cat = False):\n",
    "    # load dataset\n",
    "    df = pipe.read_data(rel_path=\"../data/uveitis_data.xlsx\")\n",
    "    \n",
    "    df = (df.pipe(pipe.rename, path=\"../data/col_names&data_type-Copy1.xlsx\") # rename columns\n",
    "        .pipe(pd.DataFrame.applymap, lambda x: x.strip() if isinstance(x, str) else x) # strip leading or trailing whitespace\n",
    "\n",
    "        # dropping columns\n",
    "        .pipe(pipe.drop_nan_columns, nan_percentage=.5, verbose = False) # drop columns with above nan_percantage missing values\n",
    "        .pipe(pd.DataFrame.drop, columns=['ehr_diagnosis'])\n",
    "        .pipe(pipe.drop_via_filter, filter_str = 'hla', verbose=False)\n",
    "        \n",
    "        .pipe(gender_dtype) # change dtype from 'gender' to catgory\n",
    "        .pipe(preprocessing_race) # collapse 'race' feature\n",
    "        .pipe(pipe.preprocessing_loc, approach='multi', verbose=False) # use approach ='binary' for binary classification\n",
    "        .pipe(pipe.preprocessing_cat)\n",
    "        .pipe(pipe.preprocessing_specific) # collapse 'specific_diagnosis'\n",
    "        .pipe(preprocessing_inflammation) # # transform collumns that contain information about severeness of inlamation\n",
    "        .pipe(preprocessing_hepatitis) # clean and binarize hepatitis-columns\n",
    "        .pipe(pipe.neg_col_to_cat, columns=['anti-ccp_ab','anti-ena_screen','antinuclear_antibody','dna_double-stranded_ab', 'rheumatoid_factor'])\n",
    "        .pipe(pipe.preprocessing_numeric, num_to_cat=num_to_cat)\n",
    "\n",
    "        # drop 'uom' amd 'range' columns after use\n",
    "        .pipe(pipe.drop_uom_and_range, verbose=False)\n",
    "        )\n",
    "    return df\n",
    "\n",
    "df = preprocessing_pipe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('max_columns', None)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_csv('../data/cleaned.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test\n",
    "(wird später umgezogen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['loc'].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X = df.drop(columns=['loc']) # features\n",
    "y = df['loc'].cat.codes # target feature\n",
    "\n",
    "d = df['loc'].cat.categories\n",
    "\n",
    "\n",
    "# train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 42, stratify = y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter for numeric and categorical features\n",
    "numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "category = ['category']\n",
    "\n",
    "numeric_features = X.select_dtypes(include=numerics).columns.tolist()\n",
    "categorical_features = X.select_dtypes(include=category).columns.tolist()\n",
    "\n",
    "\n",
    "imputer = {'categorical':{'strategy':'most_frequent', 'fill_value':'missing'}, 'numerical':{'strategy':'median', 'fill_value':'mean'}}\n",
    "imputer_encoder = pipe.impute_and_encode(categorical_features, numeric_features, imputer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sklearn imports\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree, export_graphviz\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "dectree = DecisionTreeClassifier()\n",
    "\n",
    "pipeline = Pipeline(steps=[('preprocessor', imputer_encoder),\n",
    "                      ('classifier', dectree)])\n",
    "\n",
    "# Specify the hyperparameter space\n",
    "parameters = {}\n",
    "\n",
    "# Instantiate the GridSearchCV object: cv\n",
    "cv = GridSearchCV(pipeline, parameters, cv = 5)\n",
    "\n",
    "# Fit to the training set\n",
    "t = cv.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels of the test set: y_pred\n",
    "y_pred = cv.predict(X_test)\n",
    "\n",
    "# Compute and print metrics\n",
    "print(\"Accuracy: {}\".format(cv.score(X_test, y_test)))\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "ConfusionMatrixDisplay(confusion_matrix(y_test, y_pred)).plot();\n",
    "print(cv.best_params_);\n",
    "\n",
    "plt.figure(figsize=(20,20))\n",
    "plot_tree(cv.best_estimator_['classifier'], fontsize=8)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dectree = DecisionTreeClassifier()\n",
    "\n",
    "pipeline = Pipeline(steps=[('classifier', dectree)])\n",
    "\n",
    "# Specify the hyperparameter space\n",
    "parameters = {}\n",
    "\n",
    "# Instantiate the GridSearchCV object: cv\n",
    "cv = GridSearchCV(pipeline, parameters, cv = 5)\n",
    "\n",
    "# Fit to the training set\n",
    "t = cv.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels of the test set: y_pred\n",
    "y_pred = cv.predict(X_test)\n",
    "\n",
    "# Compute and print metrics\n",
    "print(\"Accuracy: {}\".format(cv.score(X_test, y_test)))\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "ConfusionMatrixDisplay(confusion_matrix(y_test, y_pred)).plot();\n",
    "print(cv.best_params_);\n",
    "\n",
    "plt.figure(figsize=(20,20))\n",
    "plot_tree(cv.best_estimator_['classifier'], fontsize=8, )\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
